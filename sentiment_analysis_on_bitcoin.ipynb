{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXE1oxC62qDu"
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tm5qSvizmpv"
   },
   "source": [
    "Import dataset from the hugging face datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"hf://datasets/edaschau/bitcoin_news/BTC_yahoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_features = df[[\"date_time\", \"title\", \"source\"]]\n",
    "df_selected_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_news = df_selected_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = btc_news[\"date_time\"].str.split(\" \")\n",
    "btc_news[\"date\"] = split_df.str[0]\n",
    "btc_news[\"time\"] = split_df.str[1]\n",
    "\n",
    "btc_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_news[\"date\"] = pd.to_datetime(btc_news[\"date\"], format=\"%Y-%m-%d\")\n",
    "btc_news[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_news.drop(columns=[\"time\", \"date_time\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nVvXwzLQfXS"
   },
   "source": [
    "Assigning sentiment score to the article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIkRaVmqZ_Vm"
   },
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajbq0eLiaWLR"
   },
   "source": [
    "Check if GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FinBERT model with GPU support\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "finbert_pipeline = pipeline(\"sentiment-analysis\", model=model_name, batch_size=64, device=device)  # Increased batch_size\n",
    "\n",
    "# Convert titles to a list for batch processing\n",
    "titles = btc_news['title'].tolist()\n",
    "\n",
    "# Define batch size (Increase for faster processing if GPU memory allows)\n",
    "batch_size = 64\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(titles), batch_size), desc=\"Processing Batches with GPU\"):\n",
    "    batch = titles[i:i+batch_size]\n",
    "    try:\n",
    "        result_batch = finbert_pipeline(batch)\n",
    "        results.extend(result_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i} to {i+batch_size}: {str(e)}\")\n",
    "        results.extend([{'label': None, 'score': None}] * len(batch))\n",
    "\n",
    "# Convert results to DataFrame columns\n",
    "btc_news['finbert_sentiment'] = [r['label'] if r else None for r in results]\n",
    "btc_news['finbert_score'] = [r['score'] if r else None for r in results]\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "btc_news.to_csv('bitcoin_articles_with_finbert_sentiment.csv', index=False)\n",
    "\n",
    "print(\"Processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_news.finbert_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_news.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT_mbwS_nfG1"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/bitcoin_articles_with_finbert_sentiment.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MV_cx4TFtjex"
   },
   "source": [
    "Proprotion of publisher on bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = df[\"source\"].value_counts()\n",
    "percentage = (articles_df / articles_df.sum()) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_articles(row, percentage):\n",
    "  if percentage[row[\"source\"]] < 3:\n",
    "    return \"Other\"\n",
    "  else:\n",
    "    return row[\"source\"]\n",
    "\n",
    "grouped_articles = df.apply(lambda row: group_articles(row, percentage), axis=1)\n",
    "grouped_articles.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "visualize_articles = grouped_articles.value_counts()\n",
    "plt.figure(figsize =(20, 10))\n",
    "plt.pie(visualize_articles, labels = visualize_articles.index, autopct = \"%.2f%%\")\n",
    "plt.title(\"Proportion of publisher on bitcoin\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKRpwsXwuh5v"
   },
   "source": [
    "Prorortion of the news according to the months by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")\n",
    "df[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "group_by_year = df.groupby([\"year\", \"month\", \"finbert_sentiment\"]).size().reset_index(name=\"count\")\n",
    "group_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_year['month_total'] = group_by_year.groupby(['year', 'month'])['count'].transform('sum')\n",
    "group_by_year['proportion'] = group_by_year['count'] / group_by_year['month_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc = pd.read_csv(\"BTC.csv\")\n",
    "btc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc[\"date\"] = pd.to_datetime(btc[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc = btc.sort_values(by=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc[\"year\"] = btc[\"date\"].dt.year\n",
    "btc[\"month\"] = btc[\"date\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_change = btc.groupby(['year', 'month'])['close'].agg(['first', 'last']).reset_index()\n",
    "monthly_change['percent_change'] = ((monthly_change['last'] - monthly_change['first']) / monthly_change['first']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_change[monthly_change[\"year\"] == 2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = df[\"year\"].unique()\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivotting table with proportion of sentiments\n",
    "pivot = group_by_year[group_by_year['year'] == 2012].pivot_table(\n",
    "    index='month',\n",
    "    columns='finbert_sentiment',\n",
    "    values='proportion',\n",
    "    fill_value = 0)\n",
    "\n",
    "# Ensure all three sentiment columns exist\n",
    "for sentiment in ['negative', 'neutral', 'positive']:\n",
    "    if sentiment not in pivot.columns:\n",
    "        pivot[sentiment] = 0\n",
    "\n",
    "\n",
    "pivot = pivot[['negative', 'neutral', 'positive']]\n",
    "\n",
    "months = pivot.index.tolist()\n",
    "negative_vals = pivot['negative'].tolist()\n",
    "neutral_vals = pivot['neutral'].tolist()\n",
    "positive_vals = pivot['positive'].tolist()\n",
    "\n",
    "pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for year in years:\n",
    "  # Taking sample of btc prices according to the year\n",
    "  monthly_change_sample = monthly_change[monthly_change[\"year\"] == year]\n",
    "  months = monthly_change_sample[\"month\"].tolist()\n",
    "  x = np.arange(len(months))\n",
    "\n",
    "  # Pivotting table with proportion of sentiments\n",
    "  pivot = group_by_year[group_by_year['year'] == year].pivot_table(\n",
    "      index='month',\n",
    "      columns='finbert_sentiment',\n",
    "      values='proportion',\n",
    "      fill_value = 0)\n",
    "\n",
    "  # Ensure all three sentiment columns exist\n",
    "  for sentiment in ['negative', 'neutral', 'positive']:\n",
    "      if sentiment not in pivot.columns:\n",
    "          pivot[sentiment] = 0\n",
    "\n",
    "  pivot = pivot.reindex(index=range(1, 13), fill_value=0)\n",
    "  pivot = pivot[['negative', 'neutral', 'positive']]\n",
    "\n",
    "\n",
    "  negative_vals = pivot['negative'].tolist()\n",
    "  neutral_vals = pivot['neutral'].tolist()\n",
    "  positive_vals = pivot['positive'].tolist()\n",
    "\n",
    "  # Creating 2 plots with price change and proporiton of sentiments\n",
    "  fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(18, 12))\n",
    "\n",
    "  # Price change bar plot\n",
    "  dynamic_color_list = [\"green\" if x > 0 else \"red\" for x in monthly_change_sample[\"percent_change\"]]\n",
    "  ax1.bar(x, monthly_change_sample[\"percent_change\"], color=dynamic_color_list)\n",
    "  ax1.set_title(f\"Percent Change of the BTC Price {year}\")\n",
    "  ax1.set_ylabel(\"Percent Change\")\n",
    "\n",
    "  # Sentiment bar plots\n",
    "  bar_width = 0.35\n",
    "  ax2.bar(x - bar_width/2, negative_vals, width=bar_width, label='Negative', color='red')\n",
    "  ax2.bar(x + bar_width/2, positive_vals, width=bar_width, label='Positive', color='green')\n",
    "  ax2.set_ylabel(\"Sentiments Proportion\")\n",
    "  ax2.set_title(\"Proporiton of the sentiments according to the months\")\n",
    "  ax2.legend()\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibw8Xy4TW8d1"
   },
   "source": [
    "## Predictive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMbLOD-dZNIP"
   },
   "source": [
    "###Preparation of the dataset. Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "btc = pd.read_csv(\"/content/bitcoin_articles_with_finbert_sentiment.csv\")\n",
    "btc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = btc.groupby(['date', 'finbert_sentiment']).size().unstack(fill_value=0).reset_index()\n",
    "sentiment_part = summary_df[[\"date\", \"negative\", \"neutral\", \"positive\"]]\n",
    "sentiment_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_part[\"date\"] = pd.to_datetime(sentiment_part[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_part = pd.read_csv(\"BTC.csv\")\n",
    "price_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_part[\"date\"] = pd.to_datetime(price_part[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(sentiment_part,price_part, on=\"date\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"ticker\"]\n",
    "merged_df.drop(columns=columns_to_drop, inplace=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO9SHXmXck8N"
   },
   "source": [
    "Finding longest consuequent number of days with sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting values by date\n",
    "merged_df = merged_df.sort_values(by = \"date\").reset_index(drop=True)\n",
    "\n",
    "# Finding gap days between dates\n",
    "merged_df['gap_days'] = merged_df['date'].diff().dt.days\n",
    "\n",
    "# Labeling consecutive segments\n",
    "merged_df['segment_id'] = (merged_df['gap_days'] != 1).cumsum()\n",
    "\n",
    "# Finding segments with most rows\n",
    "segment_sizes = merged_df.groupby('segment_id').size()\n",
    "longest_segment_id = segment_sizes.idxmax()\n",
    "\n",
    "# Get that entire consecutive segment\n",
    "longest_segment_df = merged_df[merged_df['segment_id'] == longest_segment_id].copy()\n",
    "\n",
    "# Drop gap_days and segment_id\n",
    "longest_segment_df.drop(['gap_days', 'segment_id'], axis=1, inplace=True)\n",
    "\n",
    "longest_segment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_segment_df.to_csv(\"btc_sentiment_ohlcv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElK2tk6g8tZQ"
   },
   "source": [
    "### Preparation Dataset. Version2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XumGfYrS8zqa"
   },
   "source": [
    "### Model creation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJMtvz7ggJO2"
   },
   "source": [
    "Pipeline before fitting into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating target variable\n",
    "longest_segment_df[\"close_shift\"] = longest_segment_df[\"close\"].shift(-1)\n",
    "longest_segment_df[\"target\"] = (longest_segment_df[\"close_shift\"] > longest_segment_df[\"close\"]).astype(int)\n",
    "longest_segment_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data for scaling to avoid leaking future information\n",
    "train_data = longest_segment_df.iloc[:int(0.8 * len(longest_segment_df))]\n",
    "test_data = longest_segment_df.iloc[int(0.8 * len(longest_segment_df)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVwefZCYiysN"
   },
   "source": [
    "Scaling model using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "price_columns = ['open', 'high', 'low', 'close']\n",
    "volume_columns = ['volume']\n",
    "\n",
    "price_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "volume_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fitting scaler on the train data\n",
    "price_scaler.fit(train_data[price_columns])\n",
    "volume_scaler.fit(train_data[volume_columns])\n",
    "\n",
    "# Applying each scaler for train and test datasets\n",
    "train_data[price_columns] = price_scaler.transform(train_data[price_columns])\n",
    "train_data[volume_columns] = volume_scaler.transform(train_data[volume_columns])\n",
    "\n",
    "test_data[price_columns] = price_scaler.transform(test_data[price_columns])\n",
    "test_data[volume_columns] = volume_scaler.transform(test_data[volume_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(\"close_shift\", axis=1, inplace=True)\n",
    "test_data.drop(\"close_shift\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYtrXdfejz6a"
   },
   "source": [
    "Creating sequence with definit window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(data, feature_cols, target_col, window_size=60):\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(data) - window_size):\n",
    "        # Range of data for this window\n",
    "        seq_x = data[feature_cols].iloc[i:i+window_size].values\n",
    "        # Target is the \"day after the window\"\n",
    "        seq_y = data[target_col].iloc[i+window_size]\n",
    "\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming y_train and y_test are defined from the previous code\n",
    "# Example:\n",
    "# y_train = ...\n",
    "# y_test = ...\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_train, kde=True, label='y_train')\n",
    "sns.histplot(y_test, kde=True, label='y_test', color='orange')\n",
    "plt.xlabel('Target Variable')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Target Variable (Train vs Test)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6uLILDwv2U2"
   },
   "source": [
    "Creating model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "def create_model(X_train):\n",
    "  model = Sequential()\n",
    "\n",
    "  # Model layers\n",
    "  model.add(LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "  model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "  Dropout(0.2)\n",
    "  model.add(LSTM(units=32, return_sequences=False))\n",
    "  model.add(Dense(units=64, activation=\"relu\"))\n",
    "  Dropout(0.2)\n",
    "  model.add(Dense(units=32, activation=\"relu\"))\n",
    "  model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "  model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJVZQs44zDx_"
   },
   "source": [
    "Training loop for different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [16, 32, 64, 128]\n",
    "batch_sizes = [32, 64, 128]\n",
    "results = pd.DataFrame(columns=[\"model\", \"window_size\", \"batch_size\", \"train_accuracy\", \"val_accuracy\", \"X_test\", \"y_test\"])\n",
    "\n",
    "for window_size in window_sizes:\n",
    "  for batch_size in batch_sizes:\n",
    "\n",
    "    # Creation of the sequences according to window size\n",
    "    X_train, y_train = create_sequences(\n",
    "        data=train_data,\n",
    "        feature_cols=feature_cols,\n",
    "        target_col=target_col,\n",
    "        window_size=window_size\n",
    "    )\n",
    "    X_test, y_test = create_sequences(\n",
    "        data=test_data,\n",
    "        feature_cols=feature_cols,\n",
    "        target_col=target_col,\n",
    "        window_size=window_size\n",
    "    )\n",
    "\n",
    "    # Model creation with different split data\n",
    "    model = create_model(X_train)\n",
    "\n",
    "    # Training configuration with different batch sizes\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=30,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        validation_split=0.1\n",
    "    )\n",
    "\n",
    "    # Saving results\n",
    "    train_acc = max(history.history[\"accuracy\"])\n",
    "    val_acc   = max(history.history[\"val_accuracy\"])\n",
    "\n",
    "    row_dict = {\n",
    "            \"model\": model,\n",
    "            \"window_size\": window_size,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"X_test\" : X_test,\n",
    "            \"y_test\" : y_test\n",
    "        }\n",
    "\n",
    "    # Creating row dataframe\n",
    "    temp_df = pd.DataFrame([row_dict])\n",
    "\n",
    "    # Concatenate with the main results DataFrame\n",
    "    results = pd.concat([results, temp_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mP82PLoaxX60"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "results[\"final_accuracy\"] = None\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_accuracy(model, X_test, y_test):\n",
    "\n",
    "    # Predicts probability\n",
    "    y_prob = model.predict(X_test)  # shape: (num_samples, 1)\n",
    "\n",
    "    # If you need actual class labels (0 or 1):\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    # Calculate accuracy\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "for idx, row in results.iterrows():\n",
    "    # Grab the trained model from the row\n",
    "    model = row['model']\n",
    "\n",
    "    X_test = results[\"X_test\"][idx]\n",
    "    y_test = results[\"y_test\"][idx]\n",
    "    # Compute accuracy\n",
    "    acc = compute_accuracy(model, X_test, y_test)\n",
    "\n",
    "    # Save to 'final_accuracy' column\n",
    "    results.at[idx, 'final_accuracy'] = acc\n",
    "\n",
    "results_highest = results.sort_values(by=\"final_accuracy\", ascending=False)\n",
    "results_highest"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
